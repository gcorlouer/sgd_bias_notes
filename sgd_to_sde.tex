\documentclass[11pt]{article}

\usepackage[margin=0.5in]{geometry}  
\geometry{a4paper, margin=1in}
\usepackage{comment} % for comments
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{graphicx}
% Required packages for algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[normalem]{ulem}

\begin{document}

\section{General motivation}

The inductive biases of deep learning optimziers are not well understood. SGD and its variants (e.g. Adam) are discrete update algorithms. At the beginning of training, the model parameters are initialized from some probability distribution (e.g. some gaussian distribution). Given some initial set of parameters, the SGD update has some stochasticity caused by sampling finite batches. These two sources of randomness means that SGD could end up in different minimizers from the same initial set of parameters. We want to characterize the distribution of the parameters during and at the end of training. 

The motivation for safety is to be able to sample such distribution of parameters. If we had a good sampler of the distribution of parameters during training, we could systematically sample models that are compatible with minimizing the loss. We would have better guarantees that the sampling process is unbiased and how long it would take to sample the distribution of parameters that are accessible to the sampler. This would for example allows to determine the fraction of model minimizing the loss that are safe or unsafe and give us a lower bound on the probability of sampling a safe model in the set of degenerate solutions of the loss minimization and in particular and also help us understand the robustness of some safety property.

The simplest way to model SGD is to consider the continuous-time limit of SGD. The continuous-time limit of SGD is a stochastic differential equation (SDE) that describes the evolution of the parameters during training. Under Gaussian assumptions of the noise, this continuous-time limit leads to an SDE with a drift term and a Wiener process. Under further assuming that the noise covariance of the noise is constant, we can show that the stationary distribution of SGD obtained from Fokker-Planck is the same as the tempered Bayesian posterior distribution i.e. a Gibbs distribution with a temperature parameter.

This model of SGD has been heavily exploited in the literature, but the assumptions that are made by such models are not always valid. For example, the noise covariance is not constant (anisotropy) and SGD noise has heavy tails. Also, finite learning rate is important because of edge of stability phenomena where finite learning rate allows induced some sensitivity to the curvature in the dynamics inducing SGD to escape the sharpest minima. Subdiffusive phenomena have also been observed which can be modelled at the level of the Fokker-Planck equation by a fractional time derivative operator. 

A more general model of SGD would take into account: a central flow term to model edge of stability phenomena from finite learning rate, a Levy process with alpha stable distribution to model heavy tails of the noise, a fractional time derivative operator to model subdiffusive phenomena and a non-constant noise covariance matrix to model anisotropy of the noise.

In this work we propose to bring together a more general SDE model of SGD taking into account these different effects. We study the empirical relevance of such SDE on toy models: deep linear networks, modular addition and toy transformer models. The main hypothesis that we test are wether the distribution of parameters obtained by such model better capture the training dynamics in toy models than the Langevin SDE model of SGD which leads to a tempered posterior distribution.


\section{Introduction to Stochastic Gradient Descent}

\subsection{Problem Setup}

Let $(\mathcal{X} \times \mathcal{Y}, P)$ be a probability space, where $\mathcal{X}$ is the input space and $\mathcal{Y}$ is the output space. We observe a dataset $\mathcal{D}_n = \{(X_i, Y_i)\}_{i=1}^n$ consisting of $n$ i.i.d.\ samples from the distribution $P$.

Consider a parametric model $f: \Theta \times \mathcal{X} \to \mathcal{Y}$, where $\Theta \subseteq \mathbb{R}^d$ is the parameter space. For neural networks, $f(\theta; x)$ represents the network's output given parameters $\theta \in \Theta$ and input $x \in \mathcal{X}$.

\subsection{Loss Functions}

We define a loss function $\ell: \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}_+$ that measures the discrepancy between predictions and targets. For regression tasks, we typically use the squared loss:
$$\ell(\hat{y}, y) = \frac{1}{2}\|\hat{y} - y\|^2$$

This induces the following hierarchy of loss functions:

\begin{enumerate}
    \item \textbf{Individual sample loss}: For a single data point $(x, y)$,
    $$\ell(\theta; x, y) := \ell(f(\theta; x), y)$$
    
    \item \textbf{Empirical risk} (finite-sample loss): Over the dataset $\mathcal{D}_n$,
    $$L_n(\theta) := \frac{1}{n} \sum_{i=1}^n \ell(\theta; X_i, Y_i)$$
    
    \item \textbf{Population risk} (expected loss): Over the true distribution $P$,
    $$L(\theta) := \mathbb{E}_{(X,Y) \sim P}[\ell(\theta; X, Y)]$$
\end{enumerate}

In practice, we only have access to $L_n(\theta)$ and use it as a proxy for $L(\theta)$, which we truly wish to minimize.

\subsection{Gradients}

Define the gradients with respect to parameters $\theta$:

\begin{enumerate}
    \item \textbf{Individual gradient}: $g_i(\theta) := \nabla_\theta \ell(\theta; X_i, Y_i)$
    \item \textbf{Empirical gradient}: $g_n(\theta) := \nabla L_n(\theta) = \frac{1}{n} \sum_{i=1}^n g_i(\theta)$
    \item \textbf{Stochastic gradient}: For a batch $\mathcal{B} \subset D_n$ of size $B$ sampled uniformly at random,
    $$g_{\mathcal{B}}(\theta) := \frac{1}{B} \sum_{i \in \mathcal{B}} g_i(\theta)$$
    \item \textbf{Population gradient}: $g(\theta) := \nabla L(\theta) = \mathbb{E}_{(X,Y) \sim P}[\nabla_\theta \ell(\theta; X, Y)]$
\end{enumerate}

Note that $\mathbb{E}_{\mathcal{B}}[g_{\mathcal{B}}(\theta)] = \nabla L_n(\theta)$, making the stochastic gradient an unbiased estimator of the empirical gradient.

\subsection{The SGD Algorithm}

\textbf{Stochastic Gradient Descent (SGD)} performs the following iterative update:

\begin{algorithm}
\caption{Stochastic Gradient Descent}
\begin{algorithmic}[1]
\State Initialize $\theta_0 \in \Theta$
\For{$k = 0, 1, 2, ..., K-1$}
    \State Sample mini-batch $\mathcal{B}_k \subseteq D_n$ uniformly at random
    \State Compute stochastic gradient $g_{\mathcal{B}_k}(\theta_k)$
    \State Update: $\theta_{k+1} = \theta_k - \eta_k g_{\mathcal{B}_k}(\theta_k)$
\EndFor
\end{algorithmic}
\end{algorithm}

where $\eta_k > 0$ is the learning rate (or step size) at iteration $k$.

\subsection{Mini-batch Sampling Strategies}

There are two common sampling strategies:

\begin{enumerate}
    \item \textbf{With replacement}: Each element of $\mathcal{B}_k$ is drawn independently and uniformly from $\{1, ..., n\}$
    \item \textbf{Without replacement}: Sample $B$ distinct indices uniformly from $\{1, ..., n\}$
\end{enumerate}

The with-replacement case is often easier to analyze theoretically due to independence.

\subsection{Decomposition into Drift and Noise}

It is useful to decompose the SGD update as:
$$\theta_{k+1} = \theta_k - \eta_k g_n(\theta_k) + \eta_k \xi(\theta_k)$$
where the \textbf{gradient noise} is defined as:
$$\xi(\theta_k) := g_n(\theta_k) - g_{\mathcal{B}_k}(\theta_k)$$
This yields a decomposition into:
\begin{itemize}
    \item \textbf{Drift}: $\eta_k g_n(\theta_k)$ (deterministic gradient)
    \item \textbf{Diffusion}: $\eta_k \xi(\theta_k)$ (stochastic perturbation)
\end{itemize}

\subsection{Special Cases}

\begin{enumerate}
    \item \textbf{Full-batch gradient descent}: $\mathcal{B}_k = \{1, ..., n\}$, so $\xi_k = 0$ (deterministic)
    \item \textbf{Single-sample SGD}: $|\mathcal{B}_k| = 1$, maximum noise
    \item \textbf{Constant learning rate}: $\eta_k = \eta$ for all $k$
    \item \textbf{Decaying learning rate}: $\eta_k \to 0$ as $k \to \infty$ (e.g., $\eta_k = \eta_0/\sqrt{k+1}$)
\end{enumerate}

```latex
\subsection{Properties of the Gradient Noise}

The stochasticity in SGD arises from two distinct sources of randomness, which we now analyze.

\subsubsection{Two Sources of Gradient Noise}

\textbf{1. Sampling Noise (Empirical vs.\ Population)}

The first source of randomness comes from using a finite dataset. Even if we compute the full empirical gradient $g_n(\theta)$, it differs from the true population gradient $g(\theta)$:
$$\nu_{\text{sampling}}(\theta) := g_n(\theta) - g(\theta) = \frac{1}{n} \sum_{i=1}^n g_i(\theta) - \mathbb{E}_{(X,Y) \sim P}[\nabla_\theta \ell(\theta; X, Y)]$$

Since the data points are i.i.d.\ samples from $P$, we have:
\begin{align}
\mathbb{E}_{\mathcal{D}_n}[\nu_{\text{sampling}}(\theta)] &= \mathbb{E}_{\mathcal{D}_n}\left[\frac{1}{n} \sum_{i=1}^n g_i(\theta)\right] - g(\theta) = 0
\end{align}

The covariance is:
\begin{align}
\text{Cov}_{\mathcal{D}_n}[\nu_{\text{sampling}}(\theta)] &= \text{Cov}_{\mathcal{D}_n}\left[\frac{1}{n} \sum_{i=1}^n g_i(\theta)\right] \\
&= \frac{1}{n^2} \sum_{i=1}^n \text{Cov}_{(X_i,Y_i)}[g_i(\theta)] \\
&= \frac{1}{n} \Sigma_{\text{pop}}(\theta)
\end{align}

where $\Sigma_{\text{pop}}(\theta) := \text{Cov}_{(X,Y) \sim P}[\nabla_\theta \ell(\theta; X, Y)]$ is the population gradient covariance.

\textbf{2. Mini-batch Noise (Empirical vs.\ Batch)}

The second source of randomness comes from using mini-batches instead of the full dataset:
$$\nu_{\text{batch}}(\theta) := g_{\mathcal{B}}(\theta) - g_n(\theta) = \frac{1}{B} \sum_{i \in \mathcal{B}} g_i(\theta) - \frac{1}{n} \sum_{i=1}^n g_i(\theta)$$

For uniform sampling with replacement, conditioning on the dataset $\mathcal{D}_n$:
\begin{align}
\mathbb{E}_{\mathcal{B}}[\nu_{\text{batch}}(\theta) | \mathcal{D}_n] &= \mathbb{E}_{\mathcal{B}}\left[\frac{1}{B} \sum_{i \in \mathcal{B}} g_i(\theta)\right] - g_n(\theta) = 0
\end{align}

The conditional covariance is:
\begin{align}
\text{Cov}_{\mathcal{B}}[\nu_{\text{batch}}(\theta) | \mathcal{D}_n] &= \text{Cov}_{\mathcal{B}}\left[\frac{1}{B} \sum_{i \in \mathcal{B}} g_i(\theta) \bigg| \mathcal{D}_n\right] \\
&= \frac{1}{B} \Sigma_{\text{emp}}(\theta, \mathcal{D}_n)
\end{align}

where $\Sigma_{\text{emp}}(\theta, \mathcal{D}_n) := \frac{1}{n} \sum_{i=1}^n [g_i(\theta) - g_n(\theta)][g_i(\theta) - g_n(\theta)]^T$ is the empirical gradient covariance.

\subsubsection{Total Gradient Noise}

The total noise in SGD is:
$$\xi_{\text{total}}(\theta) := g_{\mathcal{B}}(\theta) - g(\theta) = \underbrace{g_{\mathcal{B}}(\theta) - g_n(\theta)}_{\nu_{\text{batch}}(\theta)} + \underbrace{g_n(\theta) - g(\theta)}_{\nu_{\text{sampling}}(\theta)}$$

However, in the SGD algorithm as defined, we use:
$$\xi(\theta) := g_n(\theta) - g_{\mathcal{B}}(\theta) = -\nu_{\text{batch}}(\theta)$$

This is because we can only compute deviations from the empirical gradient, not the unknown population gradient.

\subsubsection{Covariance Structure of SGD Noise}

For the noise $\xi(\theta) = g_n(\theta) - g_{\mathcal{B}}(\theta)$ used in SGD:

\textbf{Proposition 1.} \textit{Under uniform mini-batch sampling with replacement, the gradient noise has the following properties:}

\begin{enumerate}
    \item \textbf{Unbiasedness}: $\mathbb{E}_{\mathcal{B}}[\xi(\theta) | \mathcal{D}_n, \theta] = 0$
    
    \item \textbf{Covariance}: 
    $$\text{Cov}_{\mathcal{B}}[\xi(\theta) | \mathcal{D}_n, \theta] = \frac{1}{B} \Sigma_{\text{emp}}(\theta, \mathcal{D}_n)$$
    
    \item \textbf{Expected Covariance}: Taking expectation over datasets,
    $$\mathbb{E}_{\mathcal{D}_n}[\text{Cov}_{\mathcal{B}}[\xi(\theta) | \mathcal{D}_n, \theta]] = \frac{1}{B} \cdot \frac{n-1}{n} \Sigma_{\text{pop}}(\theta)$$
\end{enumerate}

\textit{Proof.} 
\begin{enumerate}
    \item Follows immediately from $\mathbb{E}_{\mathcal{B}}[g_{\mathcal{B}}(\theta)] = g_n(\theta)$.
    
    \item For sampling with replacement:
    \begin{align}
    \text{Cov}_{\mathcal{B}}[\xi(\theta) | \mathcal{D}_n] &= \text{Cov}_{\mathcal{B}}[g_{\mathcal{B}}(\theta) | \mathcal{D}_n] \\
    &= \text{Var}_{\mathcal{B}}\left[\frac{1}{B} \sum_{j=1}^B g_{I_j}(\theta) \bigg| \mathcal{D}_n\right]
    \end{align}
    where $I_j$ are i.i.d.\ uniform on $\{1, ..., n\}$. Since the $I_j$ are independent:
    \begin{align}
    &= \frac{1}{B^2} \sum_{j=1}^B \text{Var}_{I_j}[g_{I_j}(\theta) | \mathcal{D}_n] \\
    &= \frac{1}{B} \cdot \frac{1}{n} \sum_{i=1}^n [g_i(\theta) - g_n(\theta)][g_i(\theta) - g_n(\theta)]^T
    \end{align}
    
    \item Using the identity $\mathbb{E}[\Sigma_{\text{emp}}] = \frac{n-1}{n} \Sigma_{\text{pop}}$ completes the proof.
\end{enumerate}

\subsubsection{Key Observations}

\begin{enumerate}
    \item \textbf{Batch size scaling}: The noise covariance scales inversely with batch size $B$. Larger batches reduce noise.
    
    \item \textbf{Position dependence}: The covariance $\Sigma_{\text{emp}}(\theta, \mathcal{D}_n)$ depends on the current parameters $\theta$, making the noise \textit{multiplicative} rather than additive.
    
    \item \textbf{Finite sample correction}: The factor $\frac{n-1}{n}$ in the expected covariance reflects the finite dataset size. As $n \to \infty$, we recover $\mathbb{E}[\text{Cov}[\xi]] = \frac{1}{B} \Sigma_{\text{pop}}(\theta)$.
    
    \item \textbf{Sampling without replacement}: If we sample without replacement, the covariance becomes:
    $$\text{Cov}_{\mathcal{B}}[\xi(\theta) | \mathcal{D}_n] = \frac{n-B}{n-1} \cdot \frac{1}{B} \Sigma_{\text{emp}}(\theta, \mathcal{D}_n)$$
    The factor $\frac{n-B}{n-1}$ represents the finite population correction.
\end{enumerate}

\subsubsection{Estimating the Noise Covariance}

In practice, we can estimate $\Sigma_{\text{emp}}(\theta, \mathcal{D}_n)$ by:
\begin{enumerate}
    \item Computing multiple stochastic gradients $\{g_{\mathcal{B}_j}(\theta)\}_{j=1}^m$ at the same $\theta$
    \item Estimating: $\hat{\Sigma} = \frac{B}{m-1} \sum_{j=1}^m [g_{\mathcal{B}_j}(\theta) - \bar{g}][g_{\mathcal{B}_j}(\theta) - \bar{g}]^T$
\end{enumerate}

where $\bar{g} = \frac{1}{m} \sum_{j=1}^m g_{\mathcal{B}_j}(\theta)$.

This characterization of gradient noise is fundamental for understanding the continuous-time limit of SGD, as the noise covariance $\Sigma(\theta)$ directly determines the diffusion coefficient in the limiting stochastic differential equation.

\section{Continuous-time limit of SGD}

\subsection{Heuristic derivation of the continuous-time limit}
Consider the SGG update:
\begin{align}
\theta_{k+1} = \theta_k - \eta_k g_{\mathcal{B}_k}(\theta_k)
\end{align}
where $g_{\mathcal{B}_k}(\theta_k)$ is the gradient at the $k$-th iteration for batch $\mathcal{B}_k$.
Define the gradient noise as:
\begin{align}
\xi(\theta_k) := g_n(\theta_k) - g_{\mathcal{B}_k}(\theta_k)
\end{align}
where $g_n(\theta_k)$ is the empirical gradient.
The new update can be written as:
\begin{align}
\Delta\theta_{k+1} := \theta_{k+1} - \theta_k = - \eta_k g_{\mathcal{B}_k}(\theta_k) + \eta_k \xi(\theta_k)
\end{align}
The 1-sample gradient expectation is:
\begin{align}
\mathbb{E}[g_i(\theta_k)] = \frac{1}{n} \sum_{i=1}^n g_i(\theta_k) = g_n(\theta_k)
\end{align}
Hence the expectation of the noise is zero.
The 1-sample noise covariance is:
\begin{align}
\Sigma(\theta_k) := \frac{1}{n} \sum_{i=1}^n [g_i(\theta_k) - g_n(\theta_k)][g_i(\theta_k) - g_n(\theta_k)]^T
\end{align}
where $g_i(\theta_k)$ is the gradient at the $i$-th data point. The batch noise covariance is:
\begin{align}
\Sigma_{\text{batch}}(\theta_k) := \frac{1}{B^2}\sum_{i\in\mathcal{B}_k} Cov(\xi(\theta_k)) = \frac{1}{B}\Sigma(\theta_k)
\end{align}
Assume that the learning rate is independent of $k$. Assume that batch size is large enough to apply CLT and much smaller than the dataset size i.e. $B \ll n$ (\textbf{independent batches?}) and $\frac{B}{n} \ll 1$. Assume that number of samples is large enough to apply CLT. Assume that samples are i.i.d. and that the gradient update (and maybe gradient noise?) have finite variance. Under these assumptions, we can apply the CLT to the batch gradient and get:
\begin{align}
\xi(\theta_k) \sim \mathcal{N}(0, \frac{1}{B}\Sigma(\theta_k))
\end{align}
Proper scaling of the noise is crucial to apply CLT and get the continuous-time limit.
Physical intuition: the average displacement of the noise is 0 but the the typical displacement of the noise is in $\sqrt{t}$ where $t = N\eta$ for $N$ steps.
More specifically
\begin{align}
Var(\sum_{k=1}^N \eta \xi_k) & = N\eta^2\Sigma(\theta_k) = t\eta\Sigma(\theta_k)\\
std( \sum_{k=1}^N \eta \xi_k) & = \sqrt{t\eta}\sqrt{\Sigma(\theta_k)}
\end{align}
with $N = \frac{t}{\eta}$ for N steps. We see that as $\eta \to 0$, the typical displacement of the accumulated noise goes to 0.
If we use the scaling $\sqrt(\eta)$ for the noise, we get:
\begin{align}
Var(\sum_{k=1}^N \sqrt(\eta) \xi_k) & = N\eta\Sigma(\theta_k) = t\Sigma(\theta_k)\\
std( \sum_{k=1}^N \sqrt(\eta) \xi_k) & = \sqrt{t}\sqrt{\Sigma(\theta_k)}
\end{align}
With $N= t/\eta$ for N steps. We see that the noise as proper scaling as the typical displacement is in $\sqrt{t}$ and does not depends on $\eta$.
The continuous-time limit of SGD is:
\begin{align}
d\theta_t = - \nabla L_n(\theta_t) dt + \sqrt{\frac{\eta}{B}\Sigma(\theta_t)} dW_t
\end{align}
where $W_t$ is a Wiener process. Note that a more rigorourous derivation would use the Lindeberg condition and the Donsker's theorem.
\subsection{Why do we care: the tempered posterior distribution}
Using the machinery of Fokker-Planck equation, we can derive the stationary distribution of the SGD process as the tempered posterior distribution. The Fokker-Planck equation is:
\begin{align}
\frac{\partial p(\theta, t)}{\partial t} = \nabla \cdot \left( 
    \nabla L_N(\theta) p(\theta, t) + \frac{1}{2} \frac{\eta}{B} \Sigma(\theta) \nabla p(\theta, t) \right)
\end{align}
Assume that the noise covariance matrix is positive definite. Let the probability current be:
\begin{align}
j(\theta, t) = \nabla L_N(\theta) p(\theta, t) + \frac{1}{2} \frac{\eta}{B} \Sigma(\theta) \nabla p(\theta, t)
\end{align}
The stationarity condition implies that the probability current is divergence free:
\begin{align}
\nabla \cdot j(\theta, t) = 0
\end{align}
Assume a detailed balance condition i.e. $j(\theta, t) = 0$. This implies that the stationary distribution satisfies:
\begin{align}
\nabla L_N(\theta) p(\theta) + \frac{1}{2} \frac{\eta}{B} \Sigma(\theta) \nabla p(\theta) = 0
\end{align}
Assume that the noise covariance matrix is constant $\Sigma(\theta) = \Sigma$, positive and definite. Then solving the latter differential equation gives us the stationary distribution as the tempered posterior distribution:
\begin{align}
p_{\infty}(\theta) = \frac{1}{Z} \exp\left(-\beta \Sigma^{-1} L_{N}(\theta)\right)
\end{align}
where $\beta = \frac{B}{2\eta}$ is the inverse temperature.
In the case where the noise covariance matrix is not constant but still positive and definite, we get:
\begin{align}
p_{\infty}(\theta) = \frac{1}{Z}|\Sigma(\theta)|^{-1/2}\exp\left(-\frac{2B}{\eta} L_{N}(\theta)\right)
\end{align}
where $Z$ is the partition function. In that case, the stationary distribution is given by the tempered posterior i.e. up to the temperature parameter SGD is similar to Bayesian inference. This result is been investigated in more details by Mandt. The assumptions in this derivation are not realistic since the noise covariance is singular i.e. not definite positive. In fact, deep neural networks are highly degenerate and we also know that SGD favour flat minima. 
\subsection{List of assumptions}

Assumptions for the continuous-time limit of SGD:
\begin{itemize}
    \item GN satisfies the Lindeberg condition: necessary for CLT to apply. Unclear how much this matters.
    \item Gradient noise has finite variance: necessary for CLT to apply. An alternative would be to consider heavy tailed noise. It is unclear how much this matters. This is debated in the literature and it would be useful to understand better how much this matters by reviewing the literature.
    \item $\eta_k$ is independent of $k$: Uncertain but technically should not be necessary as we could have $\eta_k \to dt$ after some updates and then take the continuous-time limit if we only care about the local behaviour of SGD. Unclear how much this matters, in practice it is fairly common to have time-varying learning rate. 
    \item $\eta \to dt \ll 1$: makes the learning rate small which is central to take the continuous-time limit. This is an important difference with the practical usage of SGD. Although it might be possible to approximate the discrete time dynamics with a continuous time dynamics by using a central flows which essentially add the average jittered caused by the discrete learning rate to the continuous time dynamics. See for example this paper: \href{https://arxiv.org/abs/2410.24206}{Understanding Optimization in Deep Learning with Central Flows
    }.
    \item $B \ll n$ and $\frac{B}{n} \ll 1$: Have enough batch and data samples to apply CLT
    \item No auto-correlation in the noise: makes the noise white which enables to take the continuous-time limit. Alternative is having coloured noise. This could happens for example if the batches are not independent, which could be the case, intuitively if the batches, are large. Another example is if the sampling of the batches is not without replacement. In this case it seems that the noise is anti-correlated as argued in this \href{https://openreview.net/forum?id=1QTFfyUePA&noteId=1QTFfyUePA}{paper}. Anti-correlation could be important to generalization as the latter paper argue that it biases SGD further towards flat minima. Momentum can also introduces auto-correlation in the noise. If the noise has some auto-correlation but decaying with time, the functional CLT still can be applied and we can get to the continuous-time with a Wiener process. If not we have to consider a fractional Brownian motion. \textbf{Idea}: Could we have a fractional Levy process with heavy tail noise and long-range auto-correlation? Is it the case that in practice long-range auto-correlation matter? \textbf{Note}: Fractional Brownnian or Levy process could be another connection with fractality in the training process.
    \textbf{Question}: if heavy tailed noise is considered, does it make sense to consider auto-correlation in the noise?
    \item Samples are i.i.d.: necessary for CLT to apply
    \item For now we are not considering momentum. Unsure how it changes the analysis. For example our analysis contrasts with Adam which has a momentum and a second order correction of past gradient.
\end{itemize}
Assumptions for the stationary distribution of SGD being the tempered posterior distribution:
\begin{itemize}
    \item The noise covariance matrix is positive and definite: seems important to solve the Fokker-Planck equation as we invert the noise covariance matrix to get the stationary distribution. This assumption is violated in practice since deep neural networks are highly degenerate.
    \item The noise covariance matrix is constant: this is not necessary but makes the calculation easier.
    \item Detailed balance condition: helps with getting a first ODE for the stationary distribution which enables to get the tempered posterior distribution.
    \item According to the Helmoltz decomposition, the current can be decomposed into a curl-free part and a divergence-free part. If the noise is anistoropic then we have $\nabla \times j(\theta, t) \neq 0$ In other words, the current is not curl-free. The detailed balanced condition does not hold in that condition. When the diffusion matrix $\Sigma(\theta)$ depends on $\theta$, the stationary dynamics of SGD is an \emph{out-of-equilibrium} steady state with a non-zero \emph{solenoidal probability current}. Probability circulates in parameter space rather than remaining static, so detailed balance—and the simple Boltzmann form $p \propto e^{-\beta L_N}$—no longer hold. \textbf{Note}: I am still a bit confused about the physical intuition about this. How should i think about this probability mass orbiting along some level set of the loss?
\end{itemize}

\subsection{Ranking assumptions}
After a deep research with chatGPT \href{https://chatgpt.com/s/dr_6853372e8bf48191913b0ad54514ca1b}{here} and with elicit \href{https://elicit.com/review/6aefb134-7070-4a9f-bb6d-6f86dff484ac}{here} this is my current ranking of the assumptions:
\begin{itemize}
    \item Application of CLT from assuming gaussian gradient noise (finite variance in particular, likely that noise is not gaussian but Levy). Also an interesting point is that the heavy tailness could be anisotropic i.e. be sensitive to the degeneracies in the loss landscape.
    \item Detailed balance condition from constant noise covariance. In particular this means we must consider a curl component to the probability current.
    \item \textbf{Note}: I am a bit confused about the ranking of the next two items.
    \item Small learning rate: irrealistic but see central flows paper. Would be useful to look more into the edge of stability phenomena literature.
    \item No auto-correlation in the noise: this will be violated with momentum in practice (but maybe that's ok as long as it's not long-range auto-correlation?) \textbf{Note}: I am a bit confused about this one. I'd like to understand better violation of long-range auto-correlation. Modification might include: fractional Levy, generalized Langevin dynamics, etc. Need colored noise moodels with memory. Sampling without replacement is common introducing some auto-correlation.
    \item Batch size large enough but small compared with the dataset size seems realistic especially in large scale training.
\end{itemize}
\section{\textbf{TODO}}
\begin{itemize}
    \item \sout{Explain why we care about continuous-time limit and connection with the tempered posterior distribution (also mention invertability of the noise covariance)}
    \item \sout{Have a better understanding of the detailed balance condition and links with the curl of the probability current (leading to a non stationary distribution but to an orbit instead)}
    \item Explain the rigorous mathematical derivation of the continuous-time limit of SGD.
    \item Clarify the importance of assuming finite learning rate. Read the central flows paper. Edge of stability implies that naive continuous time limit is not valid.
    \item Relatedly to a review of edge of stability phenomena literature.
    \item Check proof of stationary distribution for the case where the noise covariance is not constant.
    \item Explain connection with SLT and spectrum between continuous limit with all assumptions and discrete SGD
    \item Include momentum considerations.
\end{itemize}
\section{Questions that we could investigate}
We could start with replacing the gaussian noise assumption with a heavy tailed noise assumption. One generic idea would be to test the assumption of heavy tailness during the training dynamics. But it seems the sort of things that has already been done. What would be interesting would be to test heavy tailness if we think of a specific hypothesis about why this is plays an important role in the training dynamics. Under what conditions can we compute the stationary distribution of SGD with heavy tailed noise? 
\\
\\
See deep research literature on heavy tailed noise from chatGPT \href{https://chatgpt.com/s/dr_6853577008f88191bba42f17cbfbbc95}{here}.
Other mention: we could also look at the learning at parity paper and compare heavy tail with non heavy tail noise. One key takeway is that there is evicence for heavy tailness, that it might be important to escape faster from shallow deep minima and saddle point and find broader flat minima which help with generalization. An implication of Levy flight will be that the probability distribution will be differed from the tempered posterior distribution and will be closer to a non equilibrium distribution. 
\\
\\
We could investigate the saddle to saddle dynamics in deep linear networks. We could in particular look at the importance of heavy tailness to favour degenerate minima. Using analytic results from deep linear networks, we could probably compare Gaussian SDE with Levy SDE and see which ones better predicts the training dynamics and the distribution after training. Could we derive some theoretical results about the stationary distribution in deep linear networks using Levy SDE? 
\\
\\
We could look at toy model of superposition and look at phase transition between geometric states and amplitude of noise. One hypothesis could be that phase transition happened when some sufficiently high amplitude noise.
\\
\\
More interestingly we could look at grokking in modular addition. One prediction would be that the noise helps the model escape toward broader and flater basins that generalize better and that this is correlated with the heavy tailness of the noise. Would need to detail the experimental protocol a bit more.
\subsection{Experiments on deep linear networks}
See chat with Opus \href{https://claude.ai/chat/ac289a72-d57a-4d4b-ad09-38f17425753c}{here}.
\section{Questions}
How does gradient clipping affects SGD noise? Does not prevent heavy tail because gradient clipping is applied to batch gradient and not to the noise. So noise could be unbounded because of large fluctuations coming from difference between batch gradient and empirical gradient.

What is the difference between the tempered posterior distribution and the Bayesian posterior?

What is the intuition behind the curl of the probability current? How much does it deviate from the tempered posterior distribution/Bayes posterior? Also how much does it matter that the current is not curl-free?

How important is considering SGD vs ADAM?

Is there a tension between having a time-fractional and a parameter-fractional fokker planck operator? Perhaps there are different regime in which subdiffusive and superdiffusive behavior matters more and we don't need to have both operators at the same time.
\section{Research Proposal}

\subsection{Motivation}

Stochastic Gradient Descent (SGD) is the central algorithm driving deep learning. While it is often modeled mathematically using simplified stochastic differential equation (SDE) approximations, these standard approximations typically rely on assumptions that are empirically invalid during actual deep neural network training. Specifically, standard assumptions include:

\begin{itemize}
\item \textbf{Gaussian Gradient Noise}: Empirically violated due to observed heavy-tailed gradient distributions.
\item \textbf{Detailed Balance (Curl-Free Probability Current)}: Empirically violated due to noise anisotropy, leading to a non-zero curl in probability currents.
\item \textbf{Small Learning Rate Approximation}: Empirically violated by observed edge-of-stability phenomena, including oscillatory dynamics.
\end{itemize}

These violations imply that the distribution of SGD trajectories significantly deviates from the classical tempered posterior (Gibbs distribution) predicted by naive SDE models.

To address these shortcomings, we propose a refined SDE model incorporating key empirically supported phenomena:

\begin{itemize}
\item \textbf{Gradient Term}: Standard loss gradient.
\item \textbf{Central Flow Term}: Capturing oscillatory dynamics and sharpness-induced flows from edge-of-stability phenomena.
\item \textbf{Levy Noise with Anisotropic Covariance}: Capturing heavy-tailed and anisotropic gradient noise empirically observed during training.
\end{itemize}

The primary motivation from an AI alignment perspective is to better characterize the inductive biases of SGD. Accurately sampling the SGD trajectory distribution is critical for understanding the robustness of safety properties. Such an approach will aid in determining the likelihood of model trajectories remaining aligned and safe upon further training, as opposed to bifurcating toward deceptive or misaligned models (or being already deceptive if most models sampled on the degenerate set of loss minimizers contain many deceptive models).

\subsection{Proposed Experiment}

\textbf{Objective:} Validate our refined SDE model by comparing the distributions it generates against actual SGD trajectory distributions and those predicted by naive Gaussian-based SDE approximations.
\\
\\
\textbf{Setup: Deep Linear Networks (DLNs)}
We focus on DLNs, as they offer analytically tractable solutions and clear characterizations of degenerate minima. We also know that it is possible to have interesting saddle-to-saddle dynamics in DLNs and the more lazy NTK regime. Furthermore, we know from the saddle to saddle dynamics that SGD favour low rank solutions during training. It would be interesting to understand the influence different terms in the SDE model on the saddle to saddle dynamics.
\\
\\
\textbf{Experimental Protocol.}
\\
\textbf{Initialization:}
\begin{itemize}
\item Saddle to saddle initialization: Initialize parameters with a small covariance such that the initial parameters are close to a saddle point.
\item More lazy NTK regime: Initialize parameters with a large covariance such that initial parameters can be close to a global minimum.
\end{itemize}

\textbf{Data generation:}
\begin{itemize}
\item Generate synthetic data as random matrices: can try normal or heavy tailed random matrices.
\end{itemize}
\textbf{Trajectory Generation:}
\begin{itemize}
\item \textbf{SGD Trajectories}: Simulate true SGD with mini-batches of synthetic data.
\item \textbf{Langevin SDE}: Standard Brownian-motion-driven Langevin dynamics with isotropic (or anisotropic) Gaussian noise and no central flow.
\item \textbf{Refined SDE}: Proposed SDE with central flow, Levy-stable noise (heavy tails), and anisotropic covariance (rotational current) reflecting different assumptions behind SGD.
\end{itemize}
\textbf{Distribution Analysis:}
\begin{itemize}
\item Estimate some distance between distributions obtained from SGD and the various SDE models.
\item Quantify the shape of distributions (e.g. covariance,skewness and higher moments).
\end{itemize}
\textbf{Distance Metrics for Comparison:}
\begin{itemize}
\item \textbf{Wasserstein-2 (Earth Mover’s Distance)}: Measure of distributional differences.
\item \textbf{Maximum Mean Discrepancy (MMD)}: Non-parametric kernel-based measure suitable for finite samples.
\item \textbf{Approximate KL Divergence}: can we estimate some empirical version?
\item Pb: not sure if any of this is tractable
\end{itemize}

\textbf{Tracking escape dynamics from saddle points.}
We could test wether heavy tailness of SDE with Levy process (using alpha stable distribution) helps to predict escaping from saddle points compared with a Gaussian SDE.
Process:
\begin{itemize}
    \item Detect saddle points (for at loss plateaus, monitor gradient norm and hessian eigenvalues as one should be negative).
    \item Collect gradient noise around saddle points from SGD and SDE models (naive vs refined)
    \item Track tail index indicator e.g. Hill estimator. Hypothesis: Heavy tailness correlates with faster escape from saddle points.
    \item We could look at escape time from saddle point vs heavy tailness and rotational current. Need to estimate escape time from saddle point but not sure how.
\end{itemize}
\subsection{Hypothesis}

\textbf{Overall main Hypothesis:} The refined SDE model sampler produces trajectory distributions that significantly better match actual SGD empirical distributions along degenerate minima than naive Gaussian-based SDE samplers.

\textbf{Evaluation:}
\begin{itemize}
\item Compute pairwise distributional distances: Actual SGD $\leftrightarrow$ Naive SDE, Actual SGD $\leftrightarrow$ Refined SDE.
\item Statistically assess the hypothesis (e.g., via bootstrapping or significance testing).
\end{itemize}

\end{document}


\section{Appendix}
\subsubsection{Detailed Derivation of Batch Noise Covariance}
Let's carefully work through the derivation of the batch noise covariance, addressing why we condition on the dataset.

\textbf{Why Condition on the Dataset?}

In SGD, there are two layers of randomness:
\begin{enumerate}
    \item \textbf{Dataset randomness}: The dataset $\mathcal{D}_n = \{(X_i, Y_i)\}_{i=1}^n$ consists of random samples from the population
    \item \textbf{Mini-batch randomness}: Given a fixed dataset, we randomly select mini-batches
\end{enumerate}

When analyzing a single SGD step, the dataset is already fixed—it's the data we have. The only randomness at each iteration comes from mini-batch selection. This is why we condition on $\mathcal{D}_n$.

\textbf{Setting up the Problem}

Fix a dataset $\mathcal{D}_n$. For this fixed dataset, we have:
\begin{itemize}
    \item Individual gradients: $g_i(\theta) = \nabla_\theta \ell(\theta; X_i, Y_i)$ (these are now \textit{fixed} functions of $\theta$)
    \item Empirical gradient: $g_n(\theta) = \frac{1}{n} \sum_{i=1}^n g_i(\theta)$ (also fixed)
\end{itemize}

The mini-batch gradient for a random batch $\mathcal{B}$ is:
$$g_{\mathcal{B}}(\theta) = \frac{1}{B} \sum_{i \in \mathcal{B}} g_i(\theta)$$

The batch noise is:
$$\nu_{\text{batch}}(\theta) = g_{\mathcal{B}}(\theta) - g_n(\theta)$$

\textbf{Computing the Conditional Expectation}

For sampling with replacement, each index in $\mathcal{B}$ is drawn independently and uniformly from $\{1, ..., n\}$. Let's denote these random indices as $I_1, ..., I_B$.

\begin{align}
\mathbb{E}_{\mathcal{B}}[\nu_{\text{batch}}(\theta) | \mathcal{D}_n] &= \mathbb{E}_{\mathcal{B}}[g_{\mathcal{B}}(\theta) - g_n(\theta) | \mathcal{D}_n] \\
&= \mathbb{E}_{\mathcal{B}}[g_{\mathcal{B}}(\theta) | \mathcal{D}_n] - g_n(\theta) \\
&= \mathbb{E}_{I_1,...,I_B}\left[\frac{1}{B} \sum_{j=1}^B g_{I_j}(\theta) \bigg| \mathcal{D}_n\right] - g_n(\theta)
\end{align}

Since each $I_j$ is uniformly distributed on $\{1, ..., n\}$:
\begin{align}
\mathbb{E}_{I_j}[g_{I_j}(\theta) | \mathcal{D}_n] &= \sum_{i=1}^n \mathbb{P}(I_j = i) \cdot g_i(\theta) \\
&= \sum_{i=1}^n \frac{1}{n} \cdot g_i(\theta) \\
&= \frac{1}{n} \sum_{i=1}^n g_i(\theta) = g_n(\theta)
\end{align}

Therefore:
\begin{align}
\mathbb{E}_{\mathcal{B}}[\nu_{\text{batch}}(\theta) | \mathcal{D}_n] &= \frac{1}{B} \sum_{j=1}^B \mathbb{E}_{I_j}[g_{I_j}(\theta) | \mathcal{D}_n] - g_n(\theta) \\
&= \frac{1}{B} \sum_{j=1}^B g_n(\theta) - g_n(\theta) \\
&= g_n(\theta) - g_n(\theta) = 0
\end{align}

\textbf{Computing the Conditional Covariance}

Now for the covariance. Since $\mathbb{E}[\nu_{\text{batch}}] = 0$:
\begin{align}
\text{Cov}_{\mathcal{B}}[\nu_{\text{batch}}(\theta) | \mathcal{D}_n] &= \mathbb{E}_{\mathcal{B}}[\nu_{\text{batch}}(\theta) \nu_{\text{batch}}(\theta)^T | \mathcal{D}_n]
\end{align}

We have:
\begin{align}
\nu_{\text{batch}}(\theta) &= g_{\mathcal{B}}(\theta) - g_n(\theta) \\
&= \frac{1}{B} \sum_{j=1}^B g_{I_j}(\theta) - g_n(\theta) \\
&= \frac{1}{B} \sum_{j=1}^B [g_{I_j}(\theta) - g_n(\theta)]
\end{align}

Therefore:
\begin{align}
\text{Cov}_{\mathcal{B}}[\nu_{\text{batch}} | \mathcal{D}_n] &= \mathbb{E}_{\mathcal{B}}\left[\left(\frac{1}{B} \sum_{j=1}^B [g_{I_j}(\theta) - g_n(\theta)]\right)\left(\frac{1}{B} \sum_{k=1}^B [g_{I_k}(\theta) - g_n(\theta)]\right)^T \bigg| \mathcal{D}_n\right]
\end{align}

Expanding:
\begin{align}
&= \frac{1}{B^2} \sum_{j=1}^B \sum_{k=1}^B \mathbb{E}_{I_j,I_k}\left[[g_{I_j}(\theta) - g_n(\theta)][g_{I_k}(\theta) - g_n(\theta)]^T | \mathcal{D}_n\right]
\end{align}

For sampling with replacement, $I_j$ and $I_k$ are independent when $j \neq k$. For $j \neq k$:
\begin{align}
&\mathbb{E}_{I_j,I_k}[[g_{I_j}(\theta) - g_n(\theta)][g_{I_k}(\theta) - g_n(\theta)]^T | \mathcal{D}_n] \\
&= \mathbb{E}_{I_j}[g_{I_j}(\theta) - g_n(\theta) | \mathcal{D}_n] \cdot \mathbb{E}_{I_k}[g_{I_k}(\theta) - g_n(\theta) | \mathcal{D}_n]^T \\
&= 0 \cdot 0^T = 0
\end{align}

For $j = k$:
\begin{align}
&\mathbb{E}_{I_j}[[g_{I_j}(\theta) - g_n(\theta)][g_{I_j}(\theta) - g_n(\theta)]^T | \mathcal{D}_n] \\
&= \sum_{i=1}^n \mathbb{P}(I_j = i) \cdot [g_i(\theta) - g_n(\theta)][g_i(\theta) - g_n(\theta)]^T \\
&= \frac{1}{n} \sum_{i=1}^n [g_i(\theta) - g_n(\theta)][g_i(\theta) - g_n(\theta)]^T \\
&= \Sigma_{\text{emp}}(\theta, \mathcal{D}_n)
\end{align}

Therefore:
\begin{align}
\text{Cov}_{\mathcal{B}}[\nu_{\text{batch}} | \mathcal{D}_n] &= \frac{1}{B^2} \sum_{j=1}^B \Sigma_{\text{emp}}(\theta, \mathcal{D}_n) \\
&= \frac{1}{B^2} \cdot B \cdot \Sigma_{\text{emp}}(\theta, \mathcal{D}_n) \\
&= \frac{1}{B} \Sigma_{\text{emp}}(\theta, \mathcal{D}_n)
\end{align}

\textbf{Key Insight}

The factor $\frac{1}{B}$ arises because:
\begin{itemize}
    \item We average $B$ independent random variables (the gradients at randomly selected indices)
    \item Each has the same variance $\Sigma_{\text{emp}}$
    \item The variance of an average of $B$ i.i.d. random variables is $\frac{1}{B}$ times the individual variance
\end{itemize}

This is why larger batch sizes reduce gradient noise—the noise variance decreases as $O(1/B)$.

\subsection{Helmoltz decomposition of probability current}

The probability current can be decomposed into a curl-free part and a divergence-free part.
\begin{align}
j(\theta, t) = j_{\text{curl}}(\theta, t) + j_{\text{div}}(\theta, t)
\end{align}
Consider the expression of the probability current:
\begin{align}
j(\theta, t) = \nabla L_N(\theta) p(\theta, t) + \frac{1}{2} \frac{\eta}{B} \Sigma(\theta) \nabla p(\theta, t)
\end{align}
Let's compute the curl of the probability current:
% Probability current
\[
j_i(\theta,t)=p(\theta,t)\,\partial_i L_N(\theta)\;+\;
\frac{\eta}{2B}\,\Sigma_{ij}(\theta)\,\partial_j p(\theta,t),
\qquad i=1,\dots,d .
\]

% Curl in 3-vector form (for d=3; in higher dimensions use the antisymmetric part of ∂_i j_j)
\begin{align}
(\nabla\times j)_k
    &=\varepsilon_{klm}\,\partial_l j_m \\[4pt]
    &=\varepsilon_{klm}\,
      \partial_l\!\Bigl[p\,\partial_m L_N\Bigr]
      +\frac{\eta}{2B}\,
       \varepsilon_{klm}\,
       \partial_l\!\Bigl[\Sigma_{mj}\,\partial_j p\Bigr] \\[4pt]
    &=\varepsilon_{klm}\,(\partial_l p)\,(\partial_m L_N)
      +\frac{\eta}{2B}\,
       \varepsilon_{klm}\,
       (\partial_l\Sigma_{mj})\,(\partial_j p) \\[4pt]
    &=\bigl(\nabla p \times \nabla L_N\bigr)_k
      +\frac{\eta}{2B}\;
       \bigl[(\nabla\Sigma)\times\nabla p\bigr]_k ,
\end{align}

where  
\(
\displaystyle
\bigl[(\nabla\Sigma)\times\nabla p\bigr]_k
      :=\varepsilon_{klm}\,(\partial_l\Sigma_{mj})\,(\partial_j p)
\)
and the term
\(\varepsilon_{klm}\,\Sigma_{mj}\,\partial_l\partial_j p\)
vanishes because it contracts an antisymmetric tensor
(\(\varepsilon_{klm}\)) with a symmetric one
(\(\partial_l\partial_j p\)).


\section{Claude questions}

Consider the SGD update:
\begin{align}
\theta_{k+1} = \theta_k - \eta_k g_{\mathcal{B}_k}(\theta_k)
\end{align}
where $g_{\mathcal{B}_k}(\theta_k)$ is the gradient at the $k$-th iteration for batch $\mathcal{B}_k$.
Define the gradient noise as:
\begin{align}
\xi(\theta_k) := g_n(\theta_k) - g_{\mathcal{B}_k}(\theta_k)
\end{align}
where $g_n(\theta_k)$ is the empirical gradient.
What are the conditions on $n$ and $B$ to apply the central limit theorem to the gradient noise?
\begin{itemize}
    \item B should remain fixed as the learning rate $\eta \to 0$
    \item Typically $B \ll n$ (batch size much smaller than dataset size)
\end{itemize}
The batch size should NOT scale with $1/\eta$
$1/\eta$
The gradient noise must have finite second moments:
$\mathbb{E}[\|\xi(\theta_k)\|^2 | \theta_k] < \infty$
For mini-batch sampling with replacement, the covariance is:
$\text{Cov}[\xi(\theta_k) | \theta_k] = \frac{1}{B} \cdot \frac{n-B}{n-1} \cdot \text{Cov}_{i \sim \text{Uniform}(1,n)}[\nabla L(\theta_k; X_i)]$
This requires individual gradients to have finite variance.
If sampling with replacement: No additional constraints beyond finite variance
If sampling without replacement: Need $B/n \ll 1$
$B/n \ll 1$ so that dependencies between batches are negligible

\end{document}