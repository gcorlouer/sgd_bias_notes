@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{cohen2024understanding,
  title={Understanding Optimization in Deep Learning with Central Flows},
  author={Cohen, Jeremy M and Damian, Alex and Talwalkar, Ameet and Kolter, Zico and Lee, Jason D},
  journal={arXiv preprint arXiv:2410.24206},
  year={2024}
}

@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}

@article{xu2024overview,
  title={Overview frequency principle/spectral bias in deep learning},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao},
  journal={Communications on Applied Mathematics and Computation},
  pages={1--38},
  year={2024},
  publisher={Springer}
}


@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29218--29230},
  year={2021}
}

@article{lyu2023implicit,
  title={Implicit bias of (stochastic) gradient descent for rank-1 linear neural network},
  author={Lyu, Bochen and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={58166--58201},
  year={2023}
}

@article{varre2024sgd,
  title={SGD vs GD: Rank Deficiency in Linear Networks},
  author={Varre, Aditya Vardhan and Sagitova, Margarita and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={60133--60161},
  year={2024}
}

@inproceedings{andriushchenko2023sgd,
  title={Sgd with large step sizes learns sparse features},
  author={Andriushchenko, Maksym and Varre, Aditya Vardhan and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={903--925},
  year={2023},
  organization={PMLR}
}

@article{chen2023stochastic,
  title={Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks},
  author={Chen, Feng and Kunin, Daniel and Yamamura, Atsushi and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={35027--35063},
  year={2023}
}

@article{barbieri2025pdesgd,
  author  = {Barbieri, Davide and Bonforte, Matteo and Ibarrondo, Peio},
  title   = {Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning Processes},
  journal = {arXiv preprint arXiv:2501.08425},
  year    = {2025}
}

@inproceedings{mori2022power,
  title={Power-law escape rate of SGD},
  author={Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
  booktitle={International Conference on Machine Learning},
  pages={15959--15975},
  year={2022},
  organization={PMLR}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{Cohen2021EdgeStability,
  title   = {Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  author  = {Cohen, Jeremy M. and Kaur, Simran and Li, Yuanzhi and Kolter, J. Zico and Talwalkar, Ameet},
  journal = {arXiv preprint arXiv:2103.00065},
  year    = {2021},
  url     = {https://arxiv.org/abs/2103.00065}
}

@inproceedings{Arora2022Understanding,
  title     = {Understanding Gradient Descent on the Edge of Stability in Deep Learning},
  author    = {Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {948--1024},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/arora22a.html}
}

@inproceedings{Li2022SharpnessEoS,
  title     = {Analyzing Sharpness along {GD} Trajectory: Progressive Sharpening and Edge of Stability},
  author    = {Li, Zhouzi and Wang, Zixuan and Li, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {9983--9994},
  year      = {2022},
  url       = {https://arxiv.org/abs/2207.12678}
}

@inproceedings{Chen2023TwoStep,
  title     = {Beyond the Edge of Stability via Two-step Gradient Updates},
  author    = {Chen, Xi and Peng, Yaqian and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  year      = {2023},
  url       = {https://arxiv.org/abs/2206.04172}
}

@article{Labarriere2024DiagonalDLN,
  title   = {Optimization Insights into Deep Diagonal Linear Networks},
  author  = {Labarrière, Hippolyte and Molinari, Cesare and Rosasco, Lorenzo and Vega, Cristian and Villa, Silvia},
  journal = {arXiv preprint arXiv:2412.16765},
  year    = {2024},
  url     = {https://arxiv.org/abs/2412.16765}
}

@article{Kalra2023Universal,
  title   = {Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos},
  author  = {Kalra, Dayal Singh and He, Tianyu and Barkeshli, Maissam},
  journal = {arXiv preprint arXiv:2311.02076},
  year    = {2023},
  url     = {https://arxiv.org/abs/2311.02076}
}

@article{Ghosh2025DeepMF,
  title   = {Learning Dynamics of Deep Linear Networks Beyond the Edge of Stability},
  author  = {Ghosh, Avrajit and Kwon, Soo Min and Wang, Rongrong and Ravishankar, Saiprasad and Qu, Qing},
  journal = {arXiv preprint arXiv:2502.20531},
  year    = {2025},
  url     = {https://arxiv.org/abs/2502.20531}
}

@inproceedings{Yoo2025SingleNeuron,
  title     = {Understanding Sharpness Dynamics in Neural Network Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More},
  author    = {Yoo, Geonhui and Song, Minhak and Yun, Chulhee},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {267},
  year      = {2025},
  url       = {https://arxiv.org/abs/2506.06940}
}


@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  pages={1--57},
  year={2018}
}

@inproceedings{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{li2020towards,
  title={Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning},
  author={Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{chou2024gradient,
  title={Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank},
  author={Chou, Hung-Hsu and Gieshoff, Carsten and Maly, Johannes and Rauhut, Holger},
  journal={Applied and Computational Harmonic Analysis},
  volume={68},
  pages={101604},
  year={2024}
}

@inproceedings{damian2022self,
  title={Self-stabilization: The implicit bias of gradient descent at the edge of stability},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason D},
  booktitle={International Conference on Learning Representations},
  year={2023}
}


@article{song2023trajectory,
  title={Trajectory alignment: Understanding the edge of stability phenomenon via bifurcation theory},
  author={Song, Minhak and Ramezani-Kebrya, Ali and Peyré, Thomas and Faghri, Arman and Kim, Sung Min and Jelassi, Samy},
  journal={arXiv preprint arXiv:2307.04204},
  year={2023}
}

@article{li2024feature,
  title={Feature averaging: An implicit bias of gradient descent leading to non-robustness in neural networks},
  author={Li, Binghui and Wei, Colin},
  journal={arXiv preprint arXiv:2410.10322},
  year={2024}
}

@inproceedings{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019}
}

@inproceedings{shah2020pitfalls,
  title={The pitfalls of simplicity bias in neural networks},
  author={Shah, Harshay and Tamuly, Kaustav and Raghunathan, Aditi and Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9573--9585},
  year={2020}
}

@article{morwani2023simplicity,
  title={Simplicity bias in 1-hidden layer neural networks},
  author={Morwani, Depen and Batra, Jatin and Jain, Prateek and Netrapalli, Praneeth},
  journal={arXiv preprint arXiv:2302.00457},
  year={2023}
}

@inproceedings{woodworth2020kernel,
  title={The kernel and rich regimes of overparameterized linear models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{pesme2021continuous,
  title={Continuous vs. discrete optimization of deep neural networks},
  author={Pesme, Scott and Flammarion, Nicolas},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1774--1785},
  year={2021}
}

@article{mulayoff2021continuous,
  title={Continuous vs. discrete optimization of deep neural networks},
  author={Mulayoff, Rotem and Michaeli, Tomer and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{cai2025implicit,
  title={The implicit bias of gradient descent on non-homogeneous neural networks},
  author={Cai, Wei and Wang, Jason and Ma, Tengyu},
  journal={arXiv preprint arXiv:2501.XXXXX},
  year={2025}
}

@inproceedings{ravi2024implicit,
  title={The implicit bias of gradient descent on separable multiclass data},
  author={Ravi, Hrithik and Clayton, Alex and Damian, Alexandru and Lee, Jason and Jayaram, Karthik},
  booktitle={Advances in Neural Information Processing Systems},
  volume={37},
  year={2024}
}