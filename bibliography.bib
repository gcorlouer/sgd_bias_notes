@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{cohen2024understanding,
  title={Understanding Optimization in Deep Learning with Central Flows},
  author={Cohen, Jeremy M and Damian, Alex and Talwalkar, Ameet and Kolter, Zico and Lee, Jason D},
  journal={arXiv preprint arXiv:2410.24206},
  year={2024}
}

@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}

@article{xu2024overview,
  title={Overview frequency principle/spectral bias in deep learning},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao},
  journal={Communications on Applied Mathematics and Computation},
  pages={1--38},
  year={2024},
  publisher={Springer}
}


@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29218--29230},
  year={2021}
}

@article{lyu2023implicit,
  title={Implicit bias of (stochastic) gradient descent for rank-1 linear neural network},
  author={Lyu, Bochen and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={58166--58201},
  year={2023}
}

@article{varre2024sgd,
  title={SGD vs GD: Rank Deficiency in Linear Networks},
  author={Varre, Aditya Vardhan and Sagitova, Margarita and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={60133--60161},
  year={2024}
}

@inproceedings{andriushchenko2023sgd,
  title={Sgd with large step sizes learns sparse features},
  author={Andriushchenko, Maksym and Varre, Aditya Vardhan and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={903--925},
  year={2023},
  organization={PMLR}
}

@article{chen2023stochastic,
  title={Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks},
  author={Chen, Feng and Kunin, Daniel and Yamamura, Atsushi and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={35027--35063},
  year={2023}
}

@article{barbieri2025pdesgd,
  author  = {Barbieri, Davide and Bonforte, Matteo and Ibarrondo, Peio},
  title   = {Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning Processes},
  journal = {arXiv preprint arXiv:2501.08425},
  year    = {2025}
}

@inproceedings{mori2022power,
  title={Power-law escape rate of SGD},
  author={Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
  booktitle={International Conference on Machine Learning},
  pages={15959--15975},
  year={2022},
  organization={PMLR}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}