@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{cohen2024understanding,
  title={Understanding Optimization in Deep Learning with Central Flows},
  author={Cohen, Jeremy M and Damian, Alex and Talwalkar, Ameet and Kolter, Zico and Lee, Jason D},
  journal={arXiv preprint arXiv:2410.24206},
  year={2024}
}

@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}

@article{xu2024overview,
  title={Overview frequency principle/spectral bias in deep learning},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao},
  journal={Communications on Applied Mathematics and Computation},
  pages={1--38},
  year={2024},
  publisher={Springer}
}


@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29218--29230},
  year={2021}
}

@article{lyu2023implicit,
  title={Implicit bias of (stochastic) gradient descent for rank-1 linear neural network},
  author={Lyu, Bochen and Zhu, Zhanxing},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={58166--58201},
  year={2023}
}

@article{varre2024sgd,
  title={SGD vs GD: Rank Deficiency in Linear Networks},
  author={Varre, Aditya Vardhan and Sagitova, Margarita and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={60133--60161},
  year={2024}
}

@inproceedings{andriushchenko2023sgd,
  title={Sgd with large step sizes learns sparse features},
  author={Andriushchenko, Maksym and Varre, Aditya Vardhan and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={903--925},
  year={2023},
  organization={PMLR}
}

@article{chen2023stochastic,
  title={Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks},
  author={Chen, Feng and Kunin, Daniel and Yamamura, Atsushi and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={35027--35063},
  year={2023}
}

@article{barbieri2025pdesgd,
  author  = {Barbieri, Davide and Bonforte, Matteo and Ibarrondo, Peio},
  title   = {Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning Processes},
  journal = {arXiv preprint arXiv:2501.08425},
  year    = {2025}
}

@inproceedings{mori2022power,
  title={Power-law escape rate of SGD},
  author={Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
  booktitle={International Conference on Machine Learning},
  pages={15959--15975},
  year={2022},
  organization={PMLR}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{Cohen2021EdgeStability,
  title   = {Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  author  = {Cohen, Jeremy M. and Kaur, Simran and Li, Yuanzhi and Kolter, J. Zico and Talwalkar, Ameet},
  journal = {arXiv preprint arXiv:2103.00065},
  year    = {2021},
  url     = {https://arxiv.org/abs/2103.00065}
}

@inproceedings{Arora2022Understanding,
  title     = {Understanding Gradient Descent on the Edge of Stability in Deep Learning},
  author    = {Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  pages     = {948--1024},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/arora22a.html}
}

@inproceedings{Li2022SharpnessEoS,
  title     = {Analyzing Sharpness along {GD} Trajectory: Progressive Sharpening and Edge of Stability},
  author    = {Li, Zhouzi and Wang, Zixuan and Li, Jian},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {9983--9994},
  year      = {2022},
  url       = {https://arxiv.org/abs/2207.12678}
}

@inproceedings{Chen2023TwoStep,
  title     = {Beyond the Edge of Stability via Two-step Gradient Updates},
  author    = {Chen, Xi and Peng, Yaqian and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {202},
  year      = {2023},
  url       = {https://arxiv.org/abs/2206.04172}
}

@article{Labarriere2024DiagonalDLN,
  title   = {Optimization Insights into Deep Diagonal Linear Networks},
  author  = {Labarri√®re, Hippolyte and Molinari, Cesare and Rosasco, Lorenzo and Vega, Cristian and Villa, Silvia},
  journal = {arXiv preprint arXiv:2412.16765},
  year    = {2024},
  url     = {https://arxiv.org/abs/2412.16765}
}

@article{Kalra2023Universal,
  title   = {Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos},
  author  = {Kalra, Dayal Singh and He, Tianyu and Barkeshli, Maissam},
  journal = {arXiv preprint arXiv:2311.02076},
  year    = {2023},
  url     = {https://arxiv.org/abs/2311.02076}
}

@article{Ghosh2025DeepMF,
  title   = {Learning Dynamics of Deep Linear Networks Beyond the Edge of Stability},
  author  = {Ghosh, Avrajit and Kwon, Soo Min and Wang, Rongrong and Ravishankar, Saiprasad and Qu, Qing},
  journal = {arXiv preprint arXiv:2502.20531},
  year    = {2025},
  url     = {https://arxiv.org/abs/2502.20531}
}

@inproceedings{Yoo2025SingleNeuron,
  title     = {Understanding Sharpness Dynamics in Neural Network Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More},
  author    = {Yoo, Geonhui and Song, Minhak and Yun, Chulhee},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  series    = {Proceedings of Machine Learning Research},
  volume    = {267},
  year      = {2025},
  url       = {https://arxiv.org/abs/2506.06940}
}

@article{Saxe2013Exact,
  title   = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author  = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  journal = {arXiv preprint arXiv:1312.6120},
  year    = {2013},
  url     = {https://arxiv.org/abs/1312.6120}
}
