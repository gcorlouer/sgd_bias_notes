\documentclass[11pt]{article}
\usepackage{natbib}
\usepackage[margin=0.5in]{geometry}  
\geometry{a4paper, margin=1in}
\usepackage{comment} % for comments
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{graphicx}
% Required packages for algorithms
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[normalem]{ulem}

% --------------------------------------------------
% Extra packages required by the protocol
% --------------------------------------------------
\usepackage{booktabs}   % professional tables
\usepackage{enumitem}   % compact lists
\usepackage{siunitx}    % scientific notation
\usepackage{physics}    % \dv, \pdv, etc.

% Custom macros used in the protocol
\newcommand{\R}{\mathbb{R}}
\newcommand{\Wteacher}{\ensuremath{W_{\mathrm{teacher}}}}
\newcommand{\Wi}{\ensuremath{W_i}}
\newcommand{\Wprod}{\ensuremath{W_N\cdots W_1}}

\begin{document}
\section*{Experiments to show implicit bias of Gradient Descent (GD)}
The simplest is to start with understanding the implicit biases of GD. One implicit bias of GD is that it is biased toward low rank solutions. This has been shown in deep linear networks by Saxe et al. 2014 studying the exact solutions of gradient flow in deep linear networks.
Main implicit biases (see \href{https://claude.ai/public/artifacts/c54b85ba-9f9b-4d01-8407-2e3a3ab4639b}{deep research}):
\begin{itemize}
    \item Low rank solutions and first learns largest eigenvalues. For ex see \href{https://arxiv.org/pdf/2011.13772}{paper}.
    \item Avoiding sharp minima thanks to edge of stability.
    \item Feature averaging; \href{https://arxiv.org/pdf/2410.10322}{here}
    \item Spectral bias: learning low frequency features first \href{https://arxiv.org/pdf/1806.08734}{paper}
    \item Directional convergence of normalized weights and alignment of gradient with weights. Also implicit bias toward max margin solutions \href{https://arxiv.org/pdf/2006.06657}{paper}
    \item Conjecture: the gradient updates are themselves low rank?
\end{itemize}
Most interesting to look into are: low rank solutions, learning more important features first, edge of stability and checking GD vs Gradient flow (continuous time limit of GD). Another important paper is the saddle-to-saddle dynamics of GD by Jacot et al.

\subsection*{Experimental Protocol: Implicit Bias Toward Low-Rank Solutions}
\label{sec:prot}

\paragraph{Overview.}
This protocol empirically demonstrates that gradient‐based optimization favours low‐rank solutions when fitting a low‐rank target matrix.  Both discrete-time \emph{gradient descent} (GD) and its continuous‐time limit \emph{gradient flow} (GF) are tested across network depths, initializations, and learning rates.

% --------------------------------------------------
\subsubsection*{Network Architecture}

\paragraph*{Teacher (Target) Network.}
\begin{itemize}[nosep]
  \item Dimension: \(n = 100\) with square matrices. If dimension is too large, use smaller matrices to run quicker experiments, can also use rectangular matrices. The important part is having a deep student network in the overparameterized regime.
  \item Rank, for example: \(r \in \{5,10,20\}\).
  \item Generate Teacher network of Depth N
\end{itemize}

\paragraph*{Student (Overparameterized) Network.}
\begin{itemize}[nosep]
  \item Deep \emph{linear} network: \(\Wprod\)
  \item Depth \(N\ge2\); the model is overparameterized for any \(N\ge2\).
\end{itemize}

%------------------------------------------------------------
% Low–rank \texttt{teacher} matrix of dimension 100
%------------------------------------------------------------
\paragraph{Step 1: Orthonormal bases.}
Generate two random orthonormal matrices
\[
  U,\,V \;\in\; \mathbb{R}^{100\times r}.
\]
Only their first \(r\) columns matter, so obtain them efficiently via a
\emph{thin} QR decomposition applied to two independent \(100\times r\)
standard-normal matrices.

\paragraph{Step 2: Singular values.}
Create a length-\(r\) vector \(\boldsymbol{\sigma}\), for example, with exponentially
decaying entries (but can make other choices)
\[
  \sigma_i \;=\; 10 \cdot 0.5^{\,i-1},
  \qquad i = 1,\dots,r,
\]
ensuring clear separation of spectral modes.

\paragraph{Step 3: Assemble the matrix.}
Define
\[
  W_{\text{teacher}}
  \;=\;
  U \,\mathrm{diag}(\boldsymbol{\sigma})\, V^{\top}.
\]
Because both \(U\) and \(V\) have orthonormal columns of length \(r\),
the product has
\[
  \operatorname{rank}\bigl(W_{\text{teacher}}\bigr) \;=\; r.
\]

% --------------------------------------------------
\subsubsection*{Initialization (Saddle–to–Saddle Regime)}
\label{sec:init}
\begin{enumerate}
  \item \textbf{Gaussian:} \(\Wi \sim \mathcal{N}\!\bigl(0,\sigma^2\bigr)\) with $\sigma=w^{-\gamma}$ with $w$ the width of the given layer and $\gamma>1$ (see saddle-to-saddle dynamics by Jacot et al. for more details).
\end{enumerate}

% --------------------------------------------------
\subsubsection*{Training Setup}

\paragraph*{Mean square error Loss.}
\[
  \mathcal{L} = \tfrac12 \norm{\Wprod - \Wteacher}_F^{2}.
\]

\paragraph*{Optimizers.}
\begin{itemize}[nosep]
  \item \textbf{Gradient Descent (GD):}\quad
        \(\Wi^{(t+1)} = \Wi^{(t)} - \eta\,\pdv{\mathcal{L}}{\Wi}\),
        \(\eta \in \{10^{-4}, 10^{-3}, 10^{-2}\}\).
  \item \textbf{Gradient Flow (GF):}\quad
        \(\Delta{\Wi}{t} = -dt\,\pdv{\mathcal{L}}{\Wi}\);
        approximate with GD using smaller dt than \(\eta\). Use Runge-Kutta 4th order method to approximate the flow.
\end{itemize}

% --------------------------------------------------
\subsubsection*{Metrics}

\paragraph*{Rank‐related quantities.}
Let $\norm{W}_*$ be the nuclear norm of $W$ (sum of singular values) and $\norm{W}_F$ be the Frobenius norm. In addition to tracking rank via non zero singular values, we also track the effective rank, the stable rank. The latter grow more smoothly and show the incremental dynamics of the rank a bit better.
\begin{align*}
  r_{\mathrm{eff}}(W) &= \frac{\norm{W}_*}{\norm{W}}, &
  r_{\mathrm{stable}}(W) &= \frac{\norm{W}_F^2}{\norm{W}^2}, &
  r_{\mathrm{num}}(W) &= \#\{\sigma_i > 10^{-6}\}.
\end{align*}

\paragraph*{Tracked every iteration.}
\begin{itemize}[nosep]
  \item Layer ranks \(r(\Wi)\), product rank \(r(\Wprod)\).
  \item Singular values of \(\Wprod\).
  \item Training loss \(\mathcal{L}\).
  \item Rank of the updated weights (\(\Delta{\Wi}\)) over time.
  \item Alignment of learned vs.\ teacher singular vectors (i.e. their angles, scalar product)
\end{itemize}

% --------------------------------------------------
\subsubsection*{Experimental Procedure}

\begin{enumerate}[nosep]
  \item Initialize network at saddle-to-saddle regime.
  \item For example, train for \num{10\,000}–\num{50\,000} iterations (until convergence). \textbf{Important: also use iterations instead of epochs to show the incremental dynamics.}
  \item Log metrics every 10–50 iterations; save checkpoints every 1000 iterations.
\end{enumerate}

\paragraph*{GD vs.\ GF comparison.}
Use identical initializations; for GF pick a small step size and plot against continuous time \(t = \eta \times \text{iteration}\). Compare rank dynamics and loss convergence for both. Run the comparison for different learning rates across different order of magnitude.

\paragraph*{Parameter sweeps.}
Keep initialization fixed. These are examples of parameters to sweep:
\[
  r\!\in\!\{5,10,20\},\;
  N\!\in\!\{2,3,4,5\},\;
  \eta\!\in\!\{10^{-4},10^{-3},10^{-2}\}.
\]

% --------------------------------------------------
\subsubsection*{Visualizations}

\begin{enumerate}[label=\textbf{\arabic*.},leftmargin=*,nosep]
  \item \textbf{Effective rank evolution:} \(r_{\mathrm{eff}}(\Wprod)\) vs.\ iteration/\(t\).
  \item \textbf{Singular value dynamics:} log–linear plot of all \(\sigma_i(t)\).
  \item \textbf{Layer‐wise rank:} subplots of each \(r(\Wi)\) over time.
  \item Also plot rank of the updated weights (\(\Delta{\Wi}\)) over time.
  \item \textbf{Loss convergence:} \(\mathcal{L}\) vs.\ iteration (log scale).
\end{enumerate}

% --------------------------------------------------
\subsubsection*{Expected Results}

\begin{itemize}[nosep]
  \item Discrete rank “plateaus’’ at 1,2,\dots,\(r\).
  \item Larger singular values converge first (bias toward important features first).
  \item GF shows smoother transitions than GD.
  \item Deeper networks exhibit stronger low-rank bias.
\end{itemize}

% --------------------------------------------------
\subsubsection*{Implementation Notes}

\paragraph*{Reproducibility.}
Fix random seeds, log hyperparameters, and version-control code.

% --------------------------------------------------
\subsubsection*{Optional Extensions}

\begin{enumerate}[label=\textbf{\Alph*.},leftmargin=*,nosep]
  \item \textbf{Lazy (NTK) regime:}
  \item \textbf{Rectangular matrices:} \(\Wteacher\in\R^{m\times n}\); track singular values.
\end{enumerate}

\end{document}